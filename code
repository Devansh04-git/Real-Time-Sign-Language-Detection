import os

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import cv2
import numpy as np
import mediapipe as mp
from tensorflow import keras
from tensorflow.keras import layers
import pickle
from pathlib import Path


class SignLanguageDetector:
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.mp_draw = mp.solutions.drawing_utils
        self.model = None
        self.labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']

    def extract_landmarks(self, hand_landmarks):
        """Extract and normalize hand landmarks"""
        landmarks = []
        for lm in hand_landmarks.landmark:
            landmarks.extend([lm.x, lm.y, lm.z])

        # Normalize landmarks
        landmarks = np.array(landmarks)
        landmarks = (landmarks - landmarks.min()) / (landmarks.max() - landmarks.min() + 1e-8)
        return landmarks

    def create_model(self, input_shape=63, num_classes=10):
        """Create a neural network model"""
        model = keras.Sequential([
            layers.Dense(128, activation='relu', input_shape=(input_shape,)),
            layers.Dropout(0.3),
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.Dense(num_classes, activation='softmax')
        ])

        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

    def load_dataset_from_images(self, dataset_path):
        """Load dataset from folder structure: dataset_path/gesture_name/*.jpg"""
        print(f"\nLoading dataset from: {dataset_path}")

        X_data = []
        y_data = []

        dataset_path = Path(dataset_path)

        if not dataset_path.exists():
            print(f"Error: Dataset path {dataset_path} does not exist!")
            return None, None

        # Get all gesture folders
        gesture_folders = [f for f in dataset_path.iterdir() if f.is_dir()]

        if not gesture_folders:
            print("No gesture folders found!")
            return None, None

        self.labels = sorted([f.name for f in gesture_folders])
        print(f"Found gestures: {self.labels}")

        for idx, gesture_name in enumerate(self.labels):
            gesture_path = dataset_path / gesture_name
            image_files = list(gesture_path.glob('*.jpg')) + list(gesture_path.glob('*.png'))

            print(f"Processing {gesture_name}: {len(image_files)} images")

            for img_file in image_files:
                img = cv2.imread(str(img_file))
                if img is None:
                    continue

                rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                results = self.hands.process(rgb_img)

                if results.multi_hand_landmarks:
                    for hand_landmarks in results.multi_hand_landmarks:
                        landmarks = self.extract_landmarks(hand_landmarks)
                        X_data.append(landmarks)
                        y_data.append(idx)
                        break  # Only use first hand detected

        if len(X_data) == 0:
            print("No hand landmarks detected in images!")
            return None, None

        print(f"\nTotal samples collected: {len(X_data)}")
        return np.array(X_data), np.array(y_data)

    def save_dataset(self, X_data, y_data, filename='training_data.npz'):
        """Save collected dataset to file"""
        np.savez(filename, X=X_data, y=y_data, labels=self.labels)
        print(f"Dataset saved to {filename}")

    def load_dataset(self, filename='training_data.npz'):
        """Load dataset from file"""
        if not os.path.exists(filename):
            print(f"Dataset file {filename} not found!")
            return None, None

        data = np.load(filename, allow_pickle=True)
        X_data = data['X']
        y_data = data['y']
        self.labels = data['labels'].tolist()

        print(f"Dataset loaded: {len(X_data)} samples, {len(self.labels)} classes")
        return X_data, y_data

    def collect_training_data(self, samples_per_class=100):
        """Collect training data from webcam"""
        print("Starting data collection...")
        print("Press 'q' to quit, 's' to skip current gesture")

        X_data = []
        y_data = []

        cap = cv2.VideoCapture(0)

        for idx, label in enumerate(self.labels):
            print(f"\nCollecting data for gesture: {label}")
            print(f"Press SPACE when ready...")

            count = 0
            ready = False

            while count < samples_per_class:
                ret, frame = cap.read()
                if not ret:
                    break

                frame = cv2.flip(frame, 1)
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.hands.process(rgb_frame)

                # Display instructions
                cv2.putText(frame, f"Gesture: {label} ({count}/{samples_per_class})",
                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(frame, "Press SPACE to start collecting",
                            (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)

                if results.multi_hand_landmarks:
                    for hand_landmarks in results.multi_hand_landmarks:
                        self.mp_draw.draw_landmarks(
                            frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)

                        if ready:
                            landmarks = self.extract_landmarks(hand_landmarks)
                            X_data.append(landmarks)
                            y_data.append(idx)
                            count += 1

                cv2.imshow('Data Collection', frame)

                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    cap.release()
                    cv2.destroyAllWindows()
                    return None, None
                elif key == ord(' '):
                    ready = True
                elif key == ord('s'):
                    break

        cap.release()
        cv2.destroyAllWindows()

        return np.array(X_data), np.array(y_data)

    def train_model(self, X_train, y_train, epochs=50):
        """Train the model"""
        print("\nTraining model...")
        self.model = self.create_model(num_classes=len(self.labels))

        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=32,
            validation_split=0.2,
            verbose=1
        )

        print("Training completed!")
        return history

    def save_model(self, model_path='sign_model.h5', labels_path='labels.pkl'):
        """Save trained model and labels"""
        if self.model:
            self.model.save(model_path)
            with open(labels_path, 'wb') as f:
                pickle.dump(self.labels, f)
            print(f"Model saved to {model_path}")

    def load_model(self, model_path='sign_model.h5', labels_path='labels.pkl'):
        """Load trained model and labels"""
        if os.path.exists(model_path):
            self.model = keras.models.load_model(model_path)
            with open(labels_path, 'rb') as f:
                self.labels = pickle.load(f)
            print(f"Model loaded from {model_path}")
            return True
        return False

    def predict_realtime(self):
        """Real-time sign language prediction"""
        if self.model is None:
            print("No model loaded! Train or load a model first.")
            return

        print("\nStarting real-time detection...")
        print("Press 'q' to quit")

        cap = cv2.VideoCapture(0)

        if not cap.isOpened():
            print("ERROR: Cannot access camera!")
            print("Please check:")
            print("1. Camera is connected")
            print("2. No other application is using the camera")
            print("3. Camera permissions are granted")
            return

        print("Camera opened successfully!")

        while True:
            ret, frame = cap.read()
            if not ret:
                print("Failed to grab frame")
                break

            frame = cv2.flip(frame, 1)
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb_frame)

            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.mp_draw.draw_landmarks(
                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)

                    landmarks = self.extract_landmarks(hand_landmarks)
                    landmarks = landmarks.reshape(1, -1)

                    prediction = self.model.predict(landmarks, verbose=0)
                    predicted_class = np.argmax(prediction)
                    confidence = prediction[0][predicted_class]

                    label = self.labels[predicted_class]

                    cv2.putText(frame, f"Sign: {label}",
                                (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
                    cv2.putText(frame, f"Confidence: {confidence:.2f}",
                                (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
            else:
                cv2.putText(frame, "No hand detected",
                            (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            cv2.imshow('Sign Language Detection', frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()


def main():
    detector = SignLanguageDetector()

    print("=" * 50)
    print("Sign Language Detection System")
    print("=" * 50)
    print("\n1. Collect training data from webcam")
    print("2. Load dataset from images folder")
    print("3. Load saved dataset file (.npz)")
    print("4. Load existing model and start detection")
    print("5. Exit")

    choice = input("\nEnter your choice (1-5): ")

    if choice == '1':
        # Collect data from webcam
        print("\nYou'll collect data for these gestures:", detector.labels)
        samples = int(input("Samples per gesture (default 100): ") or "100")

        X_train, y_train = detector.collect_training_data(samples_per_class=samples)

        if X_train is not None:
            # Save dataset
            save_data = input("\nSave dataset? (y/n): ")
            if save_data.lower() == 'y':
                detector.save_dataset(X_train, y_train)

            # Train model
            detector.train_model(X_train, y_train)
            detector.save_model()

            # Start detection
            start_detection = input("\nStart real-time detection? (y/n): ")
            if start_detection.lower() == 'y':
                detector.predict_realtime()

    elif choice == '2':
        # Load from images folder
        folder_path = input("Enter dataset folder path (e.g., ./dataset): ")
        X_train, y_train = detector.load_dataset_from_images(folder_path)

        if X_train is not None:
            # Save processed dataset
            save_data = input("\nSave processed dataset? (y/n): ")
            if save_data.lower() == 'y':
                detector.save_dataset(X_train, y_train)

            # Train model
            detector.train_model(X_train, y_train)
            detector.save_model()

            # Start detection
            start_detection = input("\nStart real-time detection? (y/n): ")
            if start_detection.lower() == 'y':
                detector.predict_realtime()

    elif choice == '3':
        # Load saved dataset
        file_path = input("Enter dataset file path (default: training_data.npz): ") or "training_data.npz"
        X_train, y_train = detector.load_dataset(file_path)

        if X_train is not None:
            # Train model
            detector.train_model(X_train, y_train)
            detector.save_model()

            # Start detection
            start_detection = input("\nStart real-time detection? (y/n): ")
            if start_detection.lower() == 'y':
                detector.predict_realtime()

    elif choice == '4':
        # Load existing model
        if detector.load_model():
            detector.predict_realtime()
        else:
            print("No saved model found! Please train a model first.")

    elif choice == '5':
        print("Exiting...")
    else:
        print("Invalid choice!")
    

if __name__ == "__main__":
    main()
